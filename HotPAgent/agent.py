import asyncio
from typing import List
from datetime import datetime
import json
from pathlib import Path

from scrapers import WeiboScraper, DouyinScraper, ZhihuScraper
from llm_analyzer import LLMAnalyzer
from scoring_engine import ScoringEngine
from models import EnrichedTopic
from config import Config
from loguru import logger
from prediction_exporter import export_prediction_events


class HotTopicAgent:
    """çƒ­æ¦œåˆ†æ Agent ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.scrapers = {
            "weibo": WeiboScraper(),
            # "douyin": DouyinScraper(),
            # "zhihu": ZhihuScraper()
        }
        self.analyzer: LLMAnalyzer = LLMAnalyzer()
        self.scoring_engine = ScoringEngine()
        
        logger.info("çƒ­æ¦œ Agent åˆå§‹åŒ–å®Œæˆ")
    
    async def scrape_all_platforms(self) -> List:
        """å¹¶å‘æŠ“å–æ‰€æœ‰å¹³å°æ•°æ®"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹æŠ“å–å¤šå¹³å°çƒ­æ¦œæ•°æ®")
        logger.info("=" * 60)
        
        tasks = []
        for platform, scraper in self.scrapers.items():
            if Config.PLATFORMS[platform]["enabled"]:
                tasks.append(scraper.fetch_hot_topics(limit=Config.FETCH_LIMIT))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_topics = []
        for result in results:
            if isinstance(result, list):
                all_topics.extend(result)
            else:
                logger.error(f"æŠ“å–å‡ºé”™: {result}")
        
        logger.info(f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(all_topics)} æ¡è¯é¢˜")
        return all_topics
    
    def deduplicate_topics(self, topics: List) -> List:
        """å»é‡ï¼šç§»é™¤é‡å¤è¯é¢˜"""
        seen_titles = set()
        unique_topics = []
        
        for topic in topics:
            title_lower = topic.title.lower()
            if title_lower not in seen_titles:
                seen_titles.add(title_lower)
                unique_topics.append(topic)
        
        removed_count = len(topics) - len(unique_topics)
        if removed_count > 0:
            logger.info(f"ğŸ§¹ å»é‡å®Œæˆï¼Œç§»é™¤ {removed_count} æ¡é‡å¤è¯é¢˜")
        
        return unique_topics
    
    async def analyze_and_score(self, topics: List) -> List[EnrichedTopic]:
        """åˆ†æå¹¶è¯„åˆ†"""
        logger.info("=" * 60)
        logger.info("ğŸ¤– å¼€å§‹ LLM è¯­ä¹‰åˆ†æ")
        logger.info("=" * 60)
        
        analyses = await self.analyzer.batch_analyze(topics)
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š å¼€å§‹è®¡ç®—ç»¼åˆè¯„åˆ†")
        logger.info("=" * 60)
        
        enriched_topics = self.scoring_engine.calculate_scores(topics, analyses)
        
        return enriched_topics
    
    def save_results(self, topics: List[EnrichedTopic], timestamp: str):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        output_dir = Path(Config.OUTPUT_DIR)
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰è¯é¢˜
        all_file = output_dir / f"all_topics_{timestamp}.json"
        with open(all_file, 'w', encoding='utf-8') as f:
            data = [t.dict() for t in topics]
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ‰€æœ‰è¯é¢˜å·²ä¿å­˜: {all_file}")

        # å¯¼å‡ºé¢„æµ‹å¸‚åœºäº‹ä»¶
        try:
            predict_file = export_prediction_events(topics, timestamp)
            logger.info(f"ğŸ§­ é¢„æµ‹äº‹ä»¶å·²ä¿å­˜: {predict_file}")
        except Exception as e:
            logger.error(f"é¢„æµ‹äº‹ä»¶å¯¼å‡ºå¤±è´¥: {e}")
        
        # ä¿å­˜çˆ†ç‚¹è¯é¢˜
        breakout_topics = [t for t in topics if t.is_breakout]
        if breakout_topics:
            breakout_file = output_dir / f"breakout_topics_{timestamp}.json"
            with open(breakout_file, 'w', encoding='utf-8') as f:
                data = [t.dict() for t in breakout_topics]
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.success(f"ğŸ”¥ çˆ†ç‚¹è¯é¢˜å·²ä¿å­˜: {breakout_file} (å…± {len(breakout_topics)} æ¡)")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = self.scoring_engine.get_statistics(topics)
        stats_file = output_dir / f"statistics_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
        
        return breakout_topics
    
    async def run(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æŠ“å–-åˆ†æ-è¯„åˆ†æµç¨‹"""
        try:
            start_time = datetime.now()
            timestamp = start_time.strftime("%Y%m%d_%H%M%S")
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ çƒ­æ¦œ Agent å¼€å§‹æ‰§è¡Œ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"{'='*60}\n")
            
            # 1. æŠ“å–æ•°æ®
            all_topics = await self.scrape_all_platforms()
            
            if not all_topics:
                logger.warning("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œæœ¬æ¬¡ä»»åŠ¡ç»“æŸ")
                return
            
            # 2. å»é‡
            unique_topics = self.deduplicate_topics(all_topics)
            
            # 3. åˆ†æå’Œè¯„åˆ†
            enriched_topics = await self.analyze_and_score(unique_topics)
            
            # 4. ä¿å­˜ç»“æœ
            breakout_topics = self.save_results(enriched_topics, timestamp)
            
            # 5. è¾“å‡ºæ‘˜è¦
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ“‹ æ‰§è¡Œæ‘˜è¦")
            logger.info(f"{'='*60}")
            logger.info(f"æ€»è¯é¢˜æ•°: {len(enriched_topics)}")
            logger.info(f"çˆ†ç‚¹è¯é¢˜æ•°: {len(breakout_topics)}")
            logger.info(f"æ‰§è¡Œè€—æ—¶: {duration:.2f} ç§’")
            logger.info(f"{'='*60}\n")
            
            # æ˜¾ç¤º Top 5 çˆ†ç‚¹è¯é¢˜
            if breakout_topics:
                logger.info("ğŸ† Top 5 çˆ†ç‚¹è¯é¢˜:")
                for i, topic in enumerate(breakout_topics[:5], 1):
                    logger.info(
                        f"{i}. [{topic.platform}] {topic.title} "
                        f"(è¯„åˆ†: {topic.total_score:.2f}, æ€§è´¨: {topic.llm_analysis.topic_nature})"
                    )
        
        except Exception as e:
            logger.exception(f"âŒ Agent æ‰§è¡Œå‡ºé”™: {e}")
